# Proposed by Mihai:

## Globally Normalized Transition-based Neural Networks
http://arxiv.org/pdf/1603.06042v1.pdf

## Question Answering with Subgraph Embeddings
http://arxiv.org/pdf/1406.3676v3.pdf

## Building Machines That Learn and Think Like People
http://arxiv.org/abs/1604.00289

## Visualizing and Understanding Neural Models in NLP
http://arxiv.org/abs/1506.01066

## XGBoost: A Scalable Tree Boosting System
http://arxiv.org/abs/1603.02754

## Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations
http://arxiv.org/abs/1603.04351

## Representing Text for Joint Embedding of Text and Knowledge Bases
http://research.microsoft.com/pubs/254916/emnlp2015kgtext.pdf

## Harnessing Deep Neural Networks with Logic Rules
http://arxiv.org/abs/1603.06318

## Probabilistic Reasoning via Deep Learning: Neural Association Models
http://arxiv.org/abs/1603.07704

## Symbolic Knowledge Extraction using Lukasiewicz Logics
https://arxiv.org/abs/1604.03099#

# Proposed by Michael:

## A Persona-based Neural Conversation Model
http://arxiv.org/pdf/1603.06155.pdf

##Neural Machine Translation By Jointly Learning to Align and Translate
The seminal paper that introduces an "attention mechanism" to sequence-to-sequence models
http://arxiv.org/pdf/1409.0473.pdf

##Learning to Tranduce with Unbounded Memory
Comes up with the idea of "neural stacks"
http://arxiv.org/pdf/1506.02516.pdf

##Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves
https://arxiv.org/pdf/1604.02038v1

##Using Sentence-Level LSTM Language Models for Script Inference
https://arxiv.org/pdf/1604.02993v1

##“Why Should I Trust You?” Explaining the Predictions of Any Classifier
http://arxiv.org/pdf/1602.04938v1.pdf

##Dialogue-based Language Learning
https://arxiv.org/pdf/1604.06045v1

##Easy-First Dependency Parsing with Hierarchical Tree LSTMs
http://arxiv.org/pdf/1603.00375v1.pdf

##Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks
http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf

# Proposed by Marco:

## Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning
http://arxiv.org/pdf/1603.07954v1.pdf

## A Fast Unified Model for Parsing and Sentence Understanding
http://arxiv.org/pdf/1603.06021.pdf

## Learning a Compositional Semantics for Freebase with an Open Predicate Vocabulary
https://aclweb.org/anthology/Q/Q15/Q15-1019.pdf

## Long Short-Term Memory-Networks for Machine Reading
https://arxiv.org/pdf/1601.06733v5

## End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures
http://arxiv.org/pdf/1601.00770v2.pdf

## Augur: Mining Human Behaviors from Fiction to Power Interactive Systems
http://hci.stanford.edu/publications/augur-chi-2016.pdf

## What we write about when we write about causality: Features of causal statements across large-scale social discourse
https://arxiv.org/abs/1604.05781

## Learning to Generate Posters of Scientific Papers
http://arxiv.org/pdf/1604.01219v1.pdf
