# Proposed by Mihai:

## Combining Natural Logic and Shallow Reasoning for Question Answering
http://nlp.stanford.edu/pubs/angeli2016naturalli.pdf

## Easy Questions First? A Case Study on Curriculum Learning for Question Answering
http://www.cs.cmu.edu/~epxing/papers/2016/Sachan_Xing_ACL16a.pdf

## Unsupervised Person Slot Filling based on Graph Mining
http://nlp.cs.rpi.edu/paper/sfgraph2016.pdf

## A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task
https://arxiv.org/abs/1606.02858

## Visualizing and Understanding Neural Models in NLP
http://arxiv.org/abs/1506.01066

## Representing Text for Joint Embedding of Text and Knowledge Bases
http://research.microsoft.com/pubs/254916/emnlp2015kgtext.pdf

## Harnessing Deep Neural Networks with Logic Rules
http://arxiv.org/abs/1603.06318

## Learning Language Games through Interaction (SKIP)
http://arxiv.org/pdf/1606.02447.pdf

## Matching Networks for One Shot Learning (SKIP)
http://arxiv.org/pdf/1606.04080v1.pdf

## Knowledge Base Unification via Sense Embeddings and Disambiguation (SKIP)
https://www.semanticscholar.org/paper/Knowledge-Base-Unification-via-Sense-Embeddings-Bovi-Anke/874b8e70bc560315c21cd2692037f079e623347d/pdf

## Globally Normalized Transition-based Neural Networks (SKIP)
http://arxiv.org/pdf/1603.06042v1.pdf

## Question Answering with Subgraph Embeddings (SKIP)
http://arxiv.org/pdf/1406.3676v3.pdf

## Building Machines That Learn and Think Like People (SKIP)
http://arxiv.org/abs/1604.00289

## XGBoost: A Scalable Tree Boosting System (SKIP)
http://arxiv.org/abs/1603.02754

## Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations (SKIP)
http://arxiv.org/abs/1603.04351

## Probabilistic Reasoning via Deep Learning: Neural Association Models (SKIP)
http://arxiv.org/abs/1603.07704

## Symbolic Knowledge Extraction using Lukasiewicz Logics (SKIP)
https://arxiv.org/abs/1604.03099#

# Proposed by Michael:

## Tutorial on Reinforcement Learning
https://www.nervanasys.com/demystifying-deep-reinforcement-learning/
http://blog.deeprobotics.es/robots,/ai,/deep/learning,/rl,/reinforcement/learning/2016/07/10/rl-tutorial/
### Playing Atari with Deep Reinforcement Learning
http://arxiv.org/pdf/1312.5602v1.pdf
### Deep Reinforcement Learning for Dialogue Generation
https://arxiv.org/pdf/1606.01541v3.pdf

##Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks
http://arxiv.org/pdf/1608.04207v1.pdf

##Linear Algebraic Structure of Word Senses, with Applications to Polysemy (SKIP)
https://arxiv.org/pdf/1601.03764v1.pdf

## “Why Should I Trust You?” Explaining the Predictions of Any Classifier
http://arxiv.org/pdf/1602.04938v1.pdf

##Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge
https://arxiv.org/pdf/1606.04422v2.pdf

## Neural Machine Translation By Jointly Learning to Align and Translate
The seminal paper that introduces an "attention mechanism" to sequence-to-sequence models
http://arxiv.org/pdf/1409.0473.pdf

## Learning to Tranduce with Unbounded Memory (SKIP)
Comes up with the idea of "neural stacks"
http://arxiv.org/pdf/1506.02516.pdf

## A Neural Knowledge Language Model
http://arxiv.org/abs/1608.00318

## A Persona-based Neural Conversation Model
http://arxiv.org/pdf/1603.06155.pdf

## Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks (SKIP)
http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf

## BLACKOUT: Speeding Up Recurrent Neural Network Language Models With Very Large Vocabularies (SKIP)
http://arxiv.org/pdf/1511.06909v7.pdf

## Deep Networks with Stochastic Depth
Summarized in this blog post: http://deliprao.com/archives/134
http://arxiv.org/abs/1603.09382v1

## Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves (SKIP)
https://arxiv.org/pdf/1604.02038v1

# Proposed by Marco:

## Constraint Detection in Natural Language Problem Descriptions
http://lia.disi.unibo.it/~ml/publications/IJCAI2016.pdf

## Sparse Word Embeddings Using L1 Regularized Online Learning (skip)
http://www.ijcai.org/Proceedings/16/Papers/414.pdf

## Knowledge Representation Learning with Entities, Attributes and Relations
http://www.ijcai.org/Proceedings/16/Papers/407.pdf

## Automatic Extraction of Causal Relations from Natural Language Texts: A Comprehensive Survey
https://arxiv.org/abs/1605.07895

## Toward Future Scenario Generation: Extracting Event Causality Exploiting Semantic Relation, Context, and Association Features
http://acl2014.org/acl2014/P14-1/pdf/P14-1093.pdf

## Concept Grounding to Multiple Knowledge Bases via Indirect Supervision
https://aclweb.org/anthology/Q/Q16/Q16-1011.pdf

## Joint Extraction of Events and Entities within a Document Context
http://www.cs.cmu.edu/~bishan/papers/joint_event_naacl16.pdf

## Bidirectional RNN for Medical Event Detection in Electronic Health Records
http://aclweb.org/anthology/N/N16/N16-1056.pdf

## Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning
http://arxiv.org/pdf/1603.07954v1.pdf

## A Fast Unified Model for Parsing and Sentence Understanding
http://arxiv.org/pdf/1603.06021.pdf

## Learning a Compositional Semantics for Freebase with an Open Predicate Vocabulary
https://aclweb.org/anthology/Q/Q15/Q15-1019.pdf

## Long Short-Term Memory-Networks for Machine Reading
https://arxiv.org/pdf/1601.06733v5

## End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures
http://arxiv.org/pdf/1601.00770v2.pdf

## Augur: Mining Human Behaviors from Fiction to Power Interactive Systems
http://hci.stanford.edu/publications/augur-chi-2016.pdf

## What we write about when we write about causality: Features of causal statements across large-scale social discourse (skip)
https://arxiv.org/abs/1604.05781

## Learning to Generate Posters of Scientific Papers (skip)
http://arxiv.org/pdf/1604.01219v1.pdf

# Proposed by Dane:

## Word Sense Disambiguation with Neural Language Models
http://arxiv.org/abs/1603.07012

## Explaining Predictions of Non-Linear Classifiers in NLP
https://arxiv.org/abs/1606.07298

## On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140

## Enriching Word Vectors with Subword Information
https://arxiv.org/abs/1607.04606


# Proposed by Gus:

## Improving Hypernymy Detection with an Integrated Path-based and Distributional Method
http://arxiv.org/abs/1603.06076

# Proposed by Carlos:

## Supersparse linear models for interpretable classification
https://arxiv.org/abs/1306.6677

## Explaining the predictions of any classifier

## Semantics derived automatically from language corpora necessarily contain human biases
http://randomwalker.info/publications/language-bias.pdf

## Visualizing and Understanding Recurrent Networks
https://arxiv.org/abs/1506.02078

