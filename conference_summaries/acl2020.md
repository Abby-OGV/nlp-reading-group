Review of some ACL2020 papers 


Inter related tasks of Natural Language Inference (NLI), Fact Verification and Recognizing Textual Entailment (RTE), saw quite a few works advancing the field in this year's ACL2020 conference. 


For NLI Ellie Pavlick argues in her work that the noise usually seen in annotation by humans in NLI tasks, is not really noise, and argue that the state of the art models don't capture these disagreements.  Also there was this paper from a Johns Hopkins’ team where they look at importance/ how state of the art models like bert react to word order in NLI. Adina Williams (from Facebook AI , who created the then state of the art MNLI dataset benchmark for NLI) had a huge ton of publications this year. There is this work where she creates two diagnostic datasets (called implicature and presupposition) which tries out the pragmatic ability of inference recognizing models. Then there is this work she did with Mohit Bansal’s team (from UNorth Carolina Chapel Hill) where they create yet another NLI dataset (adversarial based) and claim that this is the new state of the art that NLI models need to beat.

Yoav Shoham’s team from AI21 labs in tel-aviv presented sensebert, a BERT variation which learns about the supersense of a given word from wordnet and not just the masked word that bert normally does. This is relevant/similar to the figer/semantic lifting work Sandeep does in our lab. Will be interesting to see how this model reacts to delexicalixation . 

In the sub field of fact verification per se, these guys introduce kernel graph attention networks, which apparently does more fine grained evidence propagation. In this work from Belinkov’s (the guy who invented the technique of probing recently) look at bias mitigation and how it is helpful in reducing cross domain performance.In his work on natural language claims, Dan Roth`s talks about provenance of claims-how to find a chain of reliable fact verifiers. These both ideas are part of/has been touched upon by the fact verification project mithun is currently working on.

Adversarial domain aware bert is a work on domain adaptation, even though done on sentiment analysis, creates a domain aware bert by doing a post training procedure. This was the only domain adaptation related work per se that I found.

Interpretability had a field day in this year's acl. There was a full fledged [tutorial](https://slideslive.com/38928626/interpretability-and-analysis-in-neural-nlp) on interpretability from Belinkov’s team based on their acl [paper](https://www.aclweb.org/anthology/2020.acl-tutorials.1.pdf). Adina (remember I mentioned she had multiple papers this year) also had multiple works on probing. There is this work where she uses information theory in probing and this one where she uses syntactic and structural probes for understanding linguistic structure learned by models. She also has a work on comparing structural probes with syntactic probes. Both of these will be very interesting to the hardcore linguistic/parser fans (Masha, Andrew Zheng?) in our lab. Also there is this work from montreal (facebook montreal and other associated “anomalies”) where they use probes to find out how systematically do models learn language. They use NLI as an example, so this is a good paper to read if you are interested in both interpretability and natural language inference. 

Bottomline: Looks like probes are the way to go if you plan to mention even an inkling of interpretability in neural networks in your works henceforth.

Also as of July 13th 2020 most of these pre recorded presentations were and are still up in the virtual conference site (you might need an acl login. Talk to mithun if you don't have one.) I highly recommend watching these videos - take advantage of an entire ACL conference being completely online.
Also sooner or later they said they will upload them in the ACL proceedings/anthology .

~Mithun


